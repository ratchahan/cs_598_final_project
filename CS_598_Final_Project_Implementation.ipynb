{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CS 598 Final Project Implementation**"
      ],
      "metadata": {
        "id": "_jhV8X84cm7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratchahan Sujithan (rls7)\n",
        "\n",
        "\n",
        "Smarak Pattnaik\t(smarakp2)"
      ],
      "metadata": {
        "id": "mJrSPmNvkeOr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2TkC4n-j5gZ"
      },
      "source": [
        "**Graph Convolution Definition**\n",
        "\n",
        "Define a single Graph Convolution Layer as outlined in the paper with\n",
        "- constructor (__init__)\n",
        "- reset_parameters\n",
        "- forward\n",
        "- object representation function (__repr__)\n",
        "\n",
        "**For function `reset_parameters()`:**\n",
        "\n",
        "Section 5.2 - \"We initialize weights using the initialization described in Glorot & Bengio (2010) and\n",
        "accordingly (row-)normalize input feature vector\"\n",
        "\n",
        "In the paper \"Understanding the difficulty of training deep feedforward neural networks\" by Xavier Glorot and Yoshua Bengio:\n",
        "\n",
        "\"We initialized the biases to be 0 and the weights Wij at\n",
        "each layer with the following commonly used heuristic:\n",
        "\n",
        "$W_{ij} \\sim U \\left( -\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}} \\right)$\n",
        "\n",
        "where U[−a, a] is the uniform distribution in the interval\n",
        "(−a, a) and n is the size of the previous layer (the number\n",
        "of columns of W).\"\n",
        "\n",
        "**For function `forward()`:**\n",
        "\n",
        "Section 2 - \"We consider a multi-layer Graph Convolutional\n",
        "Network (GCN) with the following layer-wise propagation rule:\"\n",
        "\n",
        "$H^{(l+1)} = \\sigma\\left(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}\\right)$\n",
        "\n",
        "Section 3.2 - \"In practice, we make use of TensorFlow (Abadi et al., 2015) for an efficient GPU-based implementation2 of Eq. 9 using sparse-dense matrix multiplications.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c_ii1iCdj7D9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        # constructor loads and creates weight Parameter object\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        # load and create bias Parameter object\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # From section 5.2 - \"We initialize weights using the initialization \n",
        "    # described in Glorot & Bengio (2010) and accordingly (row-)normalize input feature vector\"\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    # From Section 3.2 - \"In practice, we make use of TensorFlow (Abadi et al., 2015) \n",
        "    # for an efficient GPU-based implementation2 of Eq. 9 using sparse-dense matrix multiplications.\"\n",
        "    def forward(self, input, adj):\n",
        "        # matrix multiplication between input and weight\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # sparse matrix multiplication between adj and the output of the previous product\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92cuOL7Gjg0e"
      },
      "source": [
        "**GCN Model Definition**\n",
        "\n",
        "Define the constructor and forward function for the full GCN as outlined in the paper using the previously defined Graph Convolution Layer.\n",
        "\n",
        "We followed the below equation given in the paper\n",
        "\n",
        "$Z = f(X,A) = softmax(\\hat{A} \\space ReLU \\space (\\hat{A} X W^{(0)})W^{(1)})$\n",
        "\n",
        "Specifically the order of operations is as follows:\n",
        "\n",
        "- Renormalization of the incoming adjacency list to prevent exploding/vanishing gradients.The adjacency matrix is normalized with a diagonal matrix where each element of the diagonal is the sum of the corresponding row in the adjacency matrix. In other words, the adjacency matrix is normalized with the degree of each corresponding node in the graph\n",
        "- Pass to the first GCN layer\n",
        "- Pass through ReLU activation function\n",
        "- Pass through dropout layer\n",
        "- Pass to the second GCN layer\n",
        "- Pass through log\\_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WYYLEnmPUVgd"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND7cFMDIlU9n"
      },
      "source": [
        "**Utils Functions**\n",
        "\n",
        "Utils functions used in the training module.\n",
        "\n",
        "The original `load_data` function was written to only handle the Cora dataset. The Cora dataset contains a .cites file (\"the citation graph of the corpus\") and a .content file (each line contains a \"unique string ID of the paper followed by binary values indicating whether each word in the vocabulary is present (indicated by 1) or absent (indicated by 0) in the paper\"). The data source that the original author provides (http://www.cs.umd.edu/~sen/lbc-proj/LBC.html) does not appear to have been maintained. Unlike the Cora dataset, the Citeseer dataset contains text in the .cites file instead of just node ids which breaks the original `load_data()` function. In addition, the Pubmed data appears to have been taken down and we were unable to find a Pubmed dataset online that had data of the same format as the Cora dataset.\n",
        "\n",
        "The DGL library contains the functions `dgl.data.citation_graph.CitationGraphDataset`, `dgl.data.PubmedGraphDataset`, `dgl.data.CiteseerGraphDataset`. These functions load the graph dataset as `Dataset` objects. We rewrote the load_dataset function to handle the `Dataset` objects and get the `adj`, `features`, `labels`, `idx_train`, `idx_val`, `idx_test` values. The function first gets the `graph`, `features`, and `labels` from the dataset using the `ndata` function. We get the adjacency matrix of the graph as a dense tensor and convert this dense tensor into a numpy array. We then turn the adjacency matrix into a scipy sparse matrix with compressed row storage format. We normalize the adjacency matrix as specified in section 2.2 of the paper. Finally, we get the training, validation, and test set indices all of which are conveniently provided by the Dataset objects. Lastly, we convert the data to PyTorch tensors and return the results.\n",
        "\n",
        "From section 2.2 - \" Repeated application of this operator can therefore lead to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. To alleviate this problem, we introduce the following renormalization trick\":\n",
        "\n",
        "$I_N + D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\rightarrow \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}, \\text{ with }\n",
        "\\tilde{A} = A + I_N \\text{ and } \\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ulj02rT3lc2Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "  \n",
        "# The original load_data function\n",
        "#def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
        "#    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "#    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "#    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "#                                        dtype=np.dtype(str))\n",
        "#    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "#    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "#\n",
        "#    # build graph\n",
        "#    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "#    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "#    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "#                                    dtype=np.int32)\n",
        "#    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "#                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "#    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "#                        shape=(labels.shape[0], labels.shape[0]),\n",
        "#                        dtype=np.float32)\n",
        "#\n",
        "#    # build symmetric adjacency matrix\n",
        "#    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "#\n",
        "#    features = normalize(features)\n",
        "#    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "#\n",
        "#    idx_train = range(140)\n",
        "#    idx_val = range(200, 500)\n",
        "#    idx_test = range(500, 1500)\n",
        "#\n",
        "#    features = torch.FloatTensor(np.array(features.todense()))\n",
        "#    labels = torch.LongTensor(np.where(labels)[1])\n",
        "#    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "#\n",
        "#    idx_train = torch.LongTensor(idx_train)\n",
        "#    idx_val = torch.LongTensor(idx_val)\n",
        "#    idx_test = torch.LongTensor(idx_test)\n",
        "#\n",
        "#    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "def load_data(dataset):\n",
        "    # Extract graph, features, and labels from the dataset\n",
        "    graph = dataset[0]\n",
        "    features = graph.ndata['feat']\n",
        "    labels = graph.ndata['label']\n",
        "    train_mask = graph.ndata['train_mask']\n",
        "    val_mask = graph.ndata['val_mask']\n",
        "    test_mask = graph.ndata['test_mask']\n",
        "\n",
        "    # Get the adjacency matrix of the graph as a dense tensor and convert this \n",
        "    # dense tensor into a numpy array\n",
        "    adj = graph.adjacency_matrix().to_dense().numpy()\n",
        "\n",
        "    # Convert to a scipy sparse matrix (CSR format)\n",
        "    adj = sp.csr_matrix(adj)\n",
        "\n",
        "    # normalize adjacency matrix and features as outlined in section 2.2\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "    features = normalize(features)\n",
        "\n",
        "    # Get training, validation, and test set indices\n",
        "    idx_train = torch.LongTensor(np.where(train_mask)[0])\n",
        "    idx_val = torch.LongTensor(np.where(val_mask)[0])\n",
        "    idx_test = torch.LongTensor(np.where(test_mask)[0])\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    features = torch.FloatTensor(features)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "# From section 2.2 - \"Repeated application of this operator can therefore lead \n",
        "# to numerical instabilities and exploding/vanishing gradients when used in a deep \n",
        "# neural network model. To alleviate this problem, we introduce the (...) renormalization trick\"\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0twNNjCRk3fv"
      },
      "source": [
        "**Training Module**\n",
        "\n",
        "We added parameters `model`, `adj`, `labels`, `features`, `idx_train`, `idx_val`, and `optimizer` to the `train()` function since we now need to support different model instances for different datasets (as opposed to the original code where we only needed to support the Cora dataset). Similarly, we added parameters `adj`, `labels`, `features`, and `idx_test` to the test() function. Now our train and test module are no longer dependent on the global definition of these values and we can more easily train and test different instances of the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "id": "iWoHLw3Gzac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3410aec-fcbc-4295-8c0d-2a4280129cad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tvjanzFrtw_3"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import dgl\n",
        "\n",
        "eval_vaidation_separately = True\n",
        "\n",
        "def train(epoch, model, adj, labels, features, idx_train, idx_val, optimizer):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if eval_vaidation_separately:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    #print('Epoch: {:04d}'.format(epoch+1),\n",
        "    #      'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "    #      'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "    #      'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "    #      'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "    #      'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test(model, adj, labels, features, idx_test):\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define function `train_and_test_model` which takes the dataset as a parameter. This function calls the `load_data` function, then defines the model and optimizer, finally it calls the `train` and `test` functions. Now we can easily create model instances and train and test those models."
      ],
      "metadata": {
        "id": "_T6f9aid4nMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_model(dataset):\n",
        "  np.random.seed(42)\n",
        "  torch.manual_seed(42)\n",
        "  torch.cuda.manual_seed(42)\n",
        "  \n",
        "\n",
        "  # Load data\n",
        "  adj, features, labels, idx_train, idx_val, idx_test = load_data(dataset)\n",
        "\n",
        "  # Model and optimizer\n",
        "  model = GCN(nfeat=features.shape[1],\n",
        "              nhid=16,\n",
        "              nclass=labels.max().item() + 1,\n",
        "              dropout=0.5)\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                        lr=0.01, weight_decay=5e-4)\n",
        "  \n",
        "  print(torch.cuda.is_available())\n",
        "  model.cuda()\n",
        "  features = features.cuda()\n",
        "  adj = adj.cuda()\n",
        "  labels = labels.cuda()\n",
        "  idx_train = idx_train.cuda()\n",
        "  idx_val = idx_val.cuda()\n",
        "  idx_test = idx_test.cuda()\n",
        "\n",
        "  # Train model\n",
        "  t_total = time.time()\n",
        "  print(\"TRAIN\")\n",
        "  for epoch in range(200):\n",
        "      train(epoch, model, adj, labels, features, idx_train, idx_val, optimizer)\n",
        "  print(\"Optimization Finished!\")\n",
        "  print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "  # Testing\n",
        "  print(\"TEST\")\n",
        "  test(model, adj, labels, features, idx_test)"
      ],
      "metadata": {
        "id": "Rc5rNcVBx7Gy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run with the three citation network datasets"
      ],
      "metadata": {
        "id": "r4GgNdNIF-ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "citeseer_dataset =  dgl.data.CiteseerGraphDataset()\n",
        "train_and_test_model(citeseer_dataset)"
      ],
      "metadata": {
        "id": "ecxTBVT7zP14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7494a2ad-d612-4ec0-f79f-785fd717d950"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-608839685969>:93: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.4275s\n",
            "TEST\n",
            "Test set results: loss= 1.0236 accuracy= 0.7050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cora_dataset =  dgl.data.CoraGraphDataset()\n",
        "train_and_test_model(cora_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTywpF9B6u3j",
        "outputId": "111a0bc1-3be5-4f23-bf8f-4880cfe353bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.0107s\n",
            "TEST\n",
            "Test set results: loss= 0.7089 accuracy= 0.8160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pubmed_dataset =  dgl.data.PubmedGraphDataset()\n",
        "train_and_test_model(pubmed_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-U9uf5260yH",
        "outputId": "c384291d-99c5-4587-c77b-0bc971e843e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.1611s\n",
            "TEST\n",
            "Test set results: loss= 0.5586 accuracy= 0.7850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woiK4IL3cTp9"
      },
      "source": [
        "**Abalation 1 - Renormalization Removal**\n",
        "\n",
        "In section 2.2 of the paper the authors use a renormalization trick to prevent exploding/vanishing gradients. The authors note that this is an important technique to maximize the accuracy of the classification results. The adjacency matrix is normalized with a diagonal matrix where each element of the diagonal is the sum of the corresponding row in the adjacency matrix. In other words the adjacency matrix is normalized with the degree of each corresponding node in the graph. We will remove this normalization and see how it impacts the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27of95DiXet"
      },
      "source": [
        "Remove the normalization from the `load_data` function. Name this new function `load_data_no_normalization`. Call this function from `train_and_test_model_no_normalization`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2HgmJjkOJMrc"
      },
      "outputs": [],
      "source": [
        "def load_data_no_normalization(dataset):\n",
        "    # Extract graph, features, and labels from the dataset\n",
        "    graph = dataset[0]\n",
        "    features = graph.ndata['feat']\n",
        "    labels = graph.ndata['label']\n",
        "    train_mask = graph.ndata['train_mask']\n",
        "    val_mask = graph.ndata['val_mask']\n",
        "    test_mask = graph.ndata['test_mask']\n",
        "\n",
        "    # Get the adjacency matrix of the graph as a dense tensor and convert this \n",
        "    # dense tensor into a numpy array\n",
        "    adj = graph.adjacency_matrix().to_dense().numpy()\n",
        "\n",
        "    # Convert to a scipy sparse matrix (CSR format)\n",
        "    adj = sp.csr_matrix(adj)\n",
        "\n",
        "    ## REMOVE THE NORMALIZATION\n",
        "    # normalize adjacency matrix and features as outlined in section 2.2\n",
        "    #adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "    #features = normalize(features)\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "\n",
        "    # Get training, validation, and test set indices\n",
        "    idx_train = torch.LongTensor(np.where(train_mask)[0])\n",
        "    idx_val = torch.LongTensor(np.where(val_mask)[0])\n",
        "    idx_test = torch.LongTensor(np.where(test_mask)[0])\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    features = torch.FloatTensor(features)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cKf8urKA9czO"
      },
      "outputs": [],
      "source": [
        "def train_and_test_model_no_normalization(dataset):\n",
        "  np.random.seed(42)\n",
        "  torch.manual_seed(42)\n",
        "  torch.cuda.manual_seed(42)\n",
        "  \n",
        "\n",
        "  # Load data\n",
        "  adj, features, labels, idx_train, idx_val, idx_test = load_data_no_normalization(dataset)\n",
        "\n",
        "  # Model and optimizer\n",
        "  model = GCN(nfeat=features.shape[1],\n",
        "              nhid=16,\n",
        "              nclass=labels.max().item() + 1,\n",
        "              dropout=0.5)\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                        lr=0.01, weight_decay=5e-4)\n",
        "  \n",
        "  print(torch.cuda.is_available())\n",
        "  model.cuda()\n",
        "  features = features.cuda()\n",
        "  adj = adj.cuda()\n",
        "  labels = labels.cuda()\n",
        "  idx_train = idx_train.cuda()\n",
        "  idx_val = idx_val.cuda()\n",
        "  idx_test = idx_test.cuda()\n",
        "\n",
        "  # Train model\n",
        "  t_total = time.time()\n",
        "  print(\"TRAIN\")\n",
        "  for epoch in range(200):\n",
        "      train(epoch, model, adj, labels, features, idx_train, idx_val, optimizer)\n",
        "  print(\"Optimization Finished!\")\n",
        "  print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "  # Testing\n",
        "  print(\"TEST\")\n",
        "  test(model, adj, labels, features, idx_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3xKJamLjUkp",
        "outputId": "ba5fe59d-6db4-48fe-e1a2-999f65f5f607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.0170s\n",
            "TEST\n",
            "Test set results: loss= 1.4504 accuracy= 0.6590\n"
          ]
        }
      ],
      "source": [
        "citeseer_dataset =  dgl.data.CiteseerGraphDataset()\n",
        "train_and_test_model_no_normalization(citeseer_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cora_dataset =  dgl.data.CoraGraphDataset()\n",
        "train_and_test_model_no_normalization(cora_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5vXWAZRmc7W",
        "outputId": "6363313b-c17c-4022-ffa7-4492e049ddeb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.0568s\n",
            "TEST\n",
            "Test set results: loss= 0.9736 accuracy= 0.7720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pubmed_dataset =  dgl.data.PubmedGraphDataset()\n",
        "train_and_test_model_no_normalization(pubmed_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HAb9CyOn0l2",
        "outputId": "b4ed6223-531c-4030-ffab-b8f2aefc3ad7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.1446s\n",
            "TEST\n",
            "Test set results: loss= 1.6247 accuracy= 0.7720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ablation 2 - Sparse Matrix Multiplication**\n",
        "\n",
        "\n",
        "The core component of the paper is that the Graph Con-\n",
        "volutional Network (GCN) provides an efficient framework\n",
        "for semi-supervised learning on graph data. The authors use\n",
        "sparse-dense matrix multiplication to minimize computational\n",
        "complexity and memory usage. For this ablation study, we\n",
        "will replace the sparse-dense matrix multiplication operation\n",
        "with dense-dense matrix multiplication operation and see if\n",
        "the results support this core argument.\n",
        "\n",
        "\n",
        "We define a new class named GraphConvolutionDense. This\n",
        "class inherits from the GraphConvolution class and overrides\n",
        "the forward function. In this new forward function, we re-\n",
        "place the sparse-dense matrix multiplication with dense-dense\n",
        "matrix multiplication. Specifically, we calculate the product\n",
        "of the input matrix and the weight matrix which outputs a\n",
        "sparse adjacency matrix. We then convert this matrix to a dense\n",
        "matrix and perform dense-dense matrix multiplication between\n",
        "this dense adjacency matrix and the product that we calculated\n",
        "in the previous step.\n",
        "\n",
        "\n",
        "Finally, we created a new load data dense function and a\n",
        "normalize dense function. The sparse adjacency matrix of the\n",
        "graph is converted to a dense matrix. We then convert this\n",
        "dense matrix into a numpy array. We apply renormalization\n",
        "using our normalize dense function. The rest of this function\n",
        "remains the same as the original load data function."
      ],
      "metadata": {
        "id": "Na1K0f2e-o62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvolutionDense(GraphConvolution):\n",
        "    # From Section 3.2 - \"In practice, we make use of TensorFlow (Abadi et al., 2015) \n",
        "    # for an efficient GPU-based implementation2 of Eq. 9 using sparse-dense matrix multiplications.\"\n",
        "    def forward(self, input, adj):\n",
        "        # matrix multiplication between input and weight\n",
        "        support = torch.mm(input, self.weight)\n",
        "\n",
        "        # USE DENSE-DENSE MATRIX MULTIPLICATION INSTEAD OF SPARSE-DENSE\n",
        "        # sparse matrix multiplication between adj and the output of the previous product\n",
        "        \n",
        "        # convert the sparse adj matrix into a dense matrix\n",
        "        adj_dense = adj.to_dense()\n",
        "        # use dense-dense matrix multiplication instead of sparse-dense\n",
        "        output = torch.mm(adj_dense, support)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ],
      "metadata": {
        "id": "BCS8NxokBwA5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolutionDense(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolutionDense(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "PR4ftb8e-vtn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading, training, and testing"
      ],
      "metadata": {
        "id": "D5UE8EM6cgCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import scipy.sparse as sp\n",
        "\n",
        "def load_data_dense(dataset):\n",
        "    # Extract graph, features, and labels from the dataset\n",
        "    graph = dataset[0]\n",
        "    features = graph.ndata['feat']\n",
        "    labels = graph.ndata['label']\n",
        "    train_mask = graph.ndata['train_mask']\n",
        "    val_mask = graph.ndata['val_mask']\n",
        "    test_mask = graph.ndata['test_mask']\n",
        "\n",
        "    # Get the adjacency matrix of the graph as a dense tensor and convert this \n",
        "    # dense tensor into a numpy array\n",
        "    adj = graph.adjacency_matrix().to_dense().numpy()\n",
        "\n",
        "    # Normalize adjacency matrix and features as outlined in section 2.2\n",
        "    adj = normalize_dense(adj + np.eye(adj.shape[0]))\n",
        "    features = normalize_dense(features)\n",
        "\n",
        "    # Get training, validation, and test set indices\n",
        "    idx_train = torch.LongTensor(np.where(train_mask)[0])\n",
        "    idx_val = torch.LongTensor(np.where(val_mask)[0])\n",
        "    idx_test = torch.LongTensor(np.where(test_mask)[0])\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    features = torch.FloatTensor(features)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    adj = torch.FloatTensor(adj)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "def normalize_dense(mx):\n",
        "    \"\"\"Row-normalize dense matrix.\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = np.diag(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "metadata": {
        "id": "lKvziIVsItQi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_dense(dataset):\n",
        "  np.random.seed(42)\n",
        "  torch.manual_seed(42)\n",
        "  torch.cuda.manual_seed(42)\n",
        "  \n",
        "\n",
        "  # Load data\n",
        "  adj, features, labels, idx_train, idx_val, idx_test = load_data_dense(dataset)\n",
        "\n",
        "  # Model and optimizer\n",
        "  model = GCN(nfeat=features.shape[1],\n",
        "              nhid=16,\n",
        "              nclass=labels.max().item() + 1,\n",
        "              dropout=0.5)\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                        lr=0.01, weight_decay=5e-4)\n",
        "  \n",
        "  print(torch.cuda.is_available())\n",
        "  model.cuda()\n",
        "  features = features.cuda()\n",
        "  adj = adj.cuda()\n",
        "  labels = labels.cuda()\n",
        "  idx_train = idx_train.cuda()\n",
        "  idx_val = idx_val.cuda()\n",
        "  idx_test = idx_test.cuda()\n",
        "\n",
        "  # Train model\n",
        "  t_total = time.time()\n",
        "  print(\"TRAIN\")\n",
        "  for epoch in range(200):\n",
        "      train(epoch, model, adj, labels, features, idx_train, idx_val, optimizer)\n",
        "  print(\"Optimization Finished!\")\n",
        "  print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "  # Testing\n",
        "  print(\"TEST\")\n",
        "  test(model, adj, labels, features, idx_test)"
      ],
      "metadata": {
        "id": "HBVHqjjdcSuZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citeseer_dataset =  dgl.data.CiteseerGraphDataset()\n",
        "train_and_test_dense(citeseer_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSwkTsviDpkl",
        "outputId": "24343e0a-6e85-4345-ce36-62493b4f59c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 3327\n",
            "  NumEdges: 9228\n",
            "  NumFeats: 3703\n",
            "  NumClasses: 6\n",
            "  NumTrainingSamples: 120\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-66521b780e5f>:37: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.6280s\n",
            "TEST\n",
            "Test set results: loss= 1.0236 accuracy= 0.7050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cora_dataset =  dgl.data.CoraGraphDataset()\n",
        "train_and_test_dense(cora_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yljBfKDxDxZO",
        "outputId": "6a100bd0-1e0e-4f51-a77c-12a33a14da41"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.4722s\n",
            "TEST\n",
            "Test set results: loss= 0.7089 accuracy= 0.8160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pubmed_dataset =  dgl.data.PubmedGraphDataset()\n",
        "train_and_test_dense(pubmed_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEwLQGh2Dz6A",
        "outputId": "36dc9898-6d32-4df8-fd7d-ab45b650932a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "True\n",
            "TRAIN\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.6824s\n",
            "TEST\n",
            "Test set results: loss= 0.5586 accuracy= 0.7850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_2S9N_J3e0Ek"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}